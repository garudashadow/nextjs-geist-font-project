import requests
from bs4 import BeautifulSoup
import pymysql
import pymongo
import time
import json
import concurrent.futures
from telegram import Bot

# ğŸ”¹ KONFIGURASI DATABASE ğŸ”¹
USE_MYSQL = True  # Jika ingin pakai MySQL, set True; jika pakai MongoDB, set False

MYSQL_CONFIG = {
    "host": "your_mysql_host",
    "user": "your_mysql_user",
    "password": "your_mysql_password",
    "database": "your_database_name"
}

MONGO_CONFIG = {
    "host": "your_mongodb_host",
    "port": 27017,
    "database": "berita_db",
    "collection": "berita"
}

# ğŸ”¹ KONFIGURASI TELEGRAM ğŸ”¹
TELEGRAM_BOT_TOKEN = "your_telegram_bot_token"
TELEGRAM_CHAT_ID = "your_chat_id"
bot = Bot(token=TELEGRAM_BOT_TOKEN)

# ğŸ”¹ DAFTAR MEDIA ONLINE INDONESIA ğŸ”¹
media_online_indonesia = {
    "Media Berita Umum": [
        "https://www.detik.com/",
        "https://www.kompas.com/",
        "https://www.tribunnews.com/",
        "https://www.tempo.co/",
        "https://www.liputan6.com/",
        "https://www.viva.co.id/",
        "https://www.republika.co.id/",
        "https://www.merdeka.com/",
        "https://www.suara.com/",
        "https://www.thejakartapost.com/",
        "https://www.sindonews.com/",
        "https://www.indopos.co.id/",
        "https://www.kumparan.com/",
        "https://news.detik.com/",
        "https://www.bantennews.co.id/",
        "https://www.medcom.id/",
        "https://www.pikiran-rakyat.com/",
        "https://www.suaramerdeka.com/",
        "https://www.riau24.com/",
        "https://www.pontianakpost.com/"
    ],
    "Media Olahraga": [
        "https://www.bola.com/",
        "https://sport.tempo.co/",
        "https://www.indosport.com/",
        "https://olahraga.kompas.com/",
        "https://www.goal.com/id",
        "https://www.liputan6.com/bola",
        "https://www.viva.co.id/bola",
        "https://www.bolanet.com/",
        "https://www.skor.id/",
        "https://sport.detik.com/",
        "https://www.bolasport.com/",
        "https://www.ligaolahraga.com/"
    ],
    "Media Bisnis dan Keuangan": [
        "https://www.bisnis.com/",
        "https://www.kontan.co.id/",
        "https://finansial.bisnis.com/",
        "https://investor.id/",
        "https://money.id/",
        "https://www.ekonomi.kompas.com/",
        "https://market.tempo.co/",
        "https://www.swa.co.id/",
        "https://www.marketeers.com/",
        "https://www.cermati.com/",
        "https://www.global.co.id/"
    ]
}

# ğŸ”¹ KATA KUNCI UNTUK FILTER BERITA ğŸ”¹
KEYWORDS = ["politik", "kriminal", "korupsi", "pemilu", "bencana", "ekonomi", "bisnis"]

# ğŸ”¹ Fungsi koneksi ke MySQL ğŸ”¹
def connect_mysql():
    return pymysql.connect(
        host=MYSQL_CONFIG["host"],
        user=MYSQL_CONFIG["user"],
        password=MYSQL_CONFIG["password"],
        database=MYSQL_CONFIG["database"],
        cursorclass=pymysql.cursors.DictCursor
    )

# ğŸ”¹ Fungsi koneksi ke MongoDB ğŸ”¹
def connect_mongodb():
    client = pymongo.MongoClient(MONGO_CONFIG["host"], MONGO_CONFIG["port"])
    return client[MONGO_CONFIG["database"]][MONGO_CONFIG["collection"]]

# ğŸ”¹ Fungsi scraping berita ğŸ”¹
def scrape_news(media_category, site_url):
    response = requests.get(site_url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(response.text, "html.parser")

    articles = []
    for berita in soup.find_all(["article", "h2", "h3"]):
        judul = berita.text.strip() if berita.text else "Tidak ada judul"
        link = berita.find("a")["href"] if berita.find("a") else "Tidak ada link"

        # Hanya simpan berita yang mengandung kata kunci
        if any(keyword.lower() in judul.lower() for keyword in KEYWORDS):
            articles.append({"judul": judul, "link": link, "kategori": media_category})
    
    return articles

# ğŸ”¹ Fungsi menyimpan data ke database ğŸ”¹
def save_to_db(news_list):
    if USE_MYSQL:
        conn = connect_mysql()
        cursor = conn.cursor()
        query = "INSERT INTO berita (judul, link, kategori) VALUES (%s, %s, %s)"
        for news in news_list:
            cursor.execute(query, (news["judul"], news["link"], news["kategori"]))
        conn.commit()
        conn.close()
    else:
        collection = connect_mongodb()
        collection.insert_many(news_list)

# ğŸ”¹ Fungsi mengirim berita ke Telegram ğŸ”¹
def send_to_telegram(news_list):
    for news in news_list:
        message = f"ğŸ“° *{news['judul']}*\nğŸ”— {news['link']}\nğŸ“¡ {news['kategori']}"
        bot.send_message(chat_id=TELEGRAM_CHAT_ID, text=message, parse_mode="Markdown")

# ğŸ”¹ Fungsi utama scraping ğŸ”¹
def scrape_all():
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = {}
        for category, sites in media_online_indonesia.items():
            for site in sites:
                futures[executor.submit(scrape_news, category, site)] = site

        all_results = []
        for future in concurrent.futures.as_completed(futures):
            site_name = futures[future]
            try:
                news_list = future.result()
                if news_list:
                    all_results.extend(news_list)
                    print(f"âœ… Berhasil scraping dari {site_name}")
            except Exception as e:
                print(f"âŒ Gagal scraping dari {site_name}: {e}")

    return all_results

# ğŸ”¹ Loop scraping otomatis setiap 30 menit ğŸ”¹
while True:
    print("\nğŸ” Memulai scraping berita...")
    all_news = scrape_all()

    if all_news:
        print(f"ğŸ“ Menyimpan {len(all_news)} berita ke database...")
        save_to_db(all_news)
        print("âœ… Data berita tersimpan.")

        print("ğŸ“¢ Mengirim berita ke Telegram...")
        send_to_telegram(all_news)
        print("âœ… Berita terkirim ke Telegram.")

    else:
        print("âš  Tidak ada berita yang cocok dengan kata kunci.")

    print("ğŸ•’ Menunggu 30 menit sebelum scraping berikutnya...\n")
    time.sleep(1800)  # Tunggu 30 menit sebelum scraping lagi
